{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d83235f4-a640-4537-ba39-8a7f75fb843f",
   "metadata": {},
   "source": [
    "# test training flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f305afd-23b6-4dce-a0df-567e2ce1a4e5",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74db885-67fa-45b5-947d-e4c074d9e7d7",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261a632b-b985-4da8-98df-d3dc40985132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import itertools\n",
    "\n",
    "from fenparser import FenParser\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982205ca-2e8e-40e4-8c26-93681f6a058b",
   "metadata": {},
   "source": [
    "### magic numbers (and strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f363d17f-3c6f-4e87-aa21-4062247f62bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is: mps\n"
     ]
    }
   ],
   "source": [
    "FEN_FILE = 'fens.txt'\n",
    "BATCH_SIZE = 32\n",
    "LR = 3e-4\n",
    "ITERATIONS = 100\n",
    "FEN_LEN = 64\n",
    "BOARD_LEN = 768\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "print(f'Device is: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf105114-814c-4725-9298-a39ce972e3ee",
   "metadata": {},
   "source": [
    "## data stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a08aab-1f3c-45d3-aefb-46f45435c18f",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "793754fe-fa7f-400a-ab51-6287c0307438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2kr2nr/pp1n1bpp/1bpq1p2/3p3P/1P1N2P1/2P1PP2/PB2N1B1/R2QK2R b - KQ\\n', '2rq1rk1/pp1nppbp/5np1/2pP4/2P1b1P1/1P2QN1P/P3BP1B/2RN1RK1 w - -\\n', 'rnbqkbn1/pppp4/3r1pp1/4p1Np/2B1P3/3P1Q2/PPP2PPP/RNB1K2R w - KQq\\n', 'r2qkb1r/p2npppp/2pp1n2/1p6/4P3/1P3Q1N/1PPP1PPP/RNB1K2R w - KQkq\\n', '5r2/B3p1b1/6kp/6n1/6p1/2P5/PP1r2PP/R3R1K1 w - -\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(FEN_FILE, 'r') as f:\n",
    "    fens = [row for row in f]\n",
    "\n",
    "print(fens[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dea89d6-e1d8-45a8-896d-9d5c30ac4c5e",
   "metadata": {},
   "source": [
    "### create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83bdfa55-d26f-4a0a-8981-1de729ce2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen_to_targets(f: str) -> torch.Tensor:\n",
    "    p = FenParser(f)\n",
    "    b = list(itertools.chain.from_iterable(p.parse()))\n",
    "    pieces = ['p', 'r', 'n', 'b', 'q', 'k', 'P', 'R', 'N', 'B', 'Q', 'K']\n",
    "    out = torch.zeros(BOARD_LEN)\n",
    "    for i in range(12):\n",
    "        target = torch.zeros(FEN_LEN)\n",
    "        target[[pieces[i] == x for x in b]] = 1\n",
    "        out[FEN_LEN*i:FEN_LEN*(i+1)] = target\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f768a6b-8e6e-442d-b948-27ce01bced8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FenDataset(Dataset):\n",
    "    def __init__(self, fens: List[str], tokenizer: BertTokenizer, max_len: int = 64):\n",
    "        self.fens = fens\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.fens)\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.fens[idx],\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        targets = fen_to_targets(self.fens[idx])\n",
    "        return {\n",
    "            'input_ids': torch.tensor(inputs['input_ids']),\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask']),\n",
    "            'token_type_ids': torch.tensor(inputs['token_type_ids']),\n",
    "            'targets': targets\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7362a6a1-2a7e-4e71-bc65-032ad6b3ff9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,   123,  1377,  1197,  1477,  1179,  1197,   120,  4329,  1475,\n",
      "         1179,  1475,  1830,  8661,   120,   122,  1830,  1643,  4426,  1475,\n",
      "         1643,  1477,   120,   124,  1643,  1495,  2101,   120,   122,  2101,\n",
      "         1475,  2249,  1477,  2101,  1475,   120,   123,  2101,  1475, 20923,\n",
      "         1477,   120,   153,  2064,  1477,  2249,  1475,  2064,  1475,   120,\n",
      "          155,  1477,  4880,  2428,  1477,  2069,   171,   118,   148,  4880,\n",
      "          102,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'targets': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csw/me/chess/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2618: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "split = int(len(fens) * 0.9)\n",
    "train_split = fens[:split]\n",
    "test_split = fens[split:]\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "train_ds = FenDataset(train_split, tokenizer, FEN_LEN)\n",
    "val_ds = FenDataset(test_split, tokenizer, FEN_LEN)\n",
    "\n",
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281b9f7-e835-4f1a-bc1a-23a6002e1846",
   "metadata": {},
   "source": [
    "### create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7322cc0e-6c6d-4d42-a847-2cb1fd86fbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101,  129,  120,  ...,    0,    0,    0],\n",
      "        [ 101,  187, 1179,  ...,    0,    0,    0],\n",
      "        [ 101,  187, 1477,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  124, 1197,  ...,    0,    0,    0],\n",
      "        [ 101,  187, 1477,  ...,    0,    0,    0],\n",
      "        [ 101,  123, 1197,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'targets': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n"
     ]
    }
   ],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(next(iter(train_dl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53b1ff-b230-44bc-837c-6ba934531130",
   "metadata": {},
   "source": [
    "## model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbb7064-6236-42f5-bb0f-a5b2139ee61a",
   "metadata": {},
   "source": [
    "### model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1288b2f8-0783-48df-a38c-e1484b9e5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FenModel(nn.Module):\n",
    "    def __init__(self, out_size: int = BOARD_LEN, dropout: bool = False):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.bert.pooler = nn.Linear(BOARD_LEN, BOARD_LEN)\n",
    "        self.linear = nn.Linear(BOARD_LEN, out_size)\n",
    "        if not dropout:\n",
    "            self.dropout = nn.Dropout(0.2)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]):\n",
    "        x = self.bert(\n",
    "            input_ids=x['input_ids'],\n",
    "            attention_mask=x['attention_mask'],\n",
    "            token_type_ids=x['token_type_ids'],\n",
    "            return_dict=False\n",
    "        )[0][:,0]\n",
    "        if self.dropout is not None:\n",
    "            x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a978a8da-5961-4fa8-919f-ea3034235a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.4766e-01,  8.8781e-02, -6.9183e-01,  ..., -8.8613e-02,\n",
       "         -4.9581e-01,  5.7306e-01],\n",
       "        [-7.4112e-01, -1.0786e-01, -2.2075e-01,  ...,  1.2325e-01,\n",
       "         -2.7112e-01,  1.6935e-01],\n",
       "        [-2.5089e-01, -6.2655e-02, -9.7424e-01,  ...,  1.2594e-02,\n",
       "         -1.6093e-01,  3.7004e-01],\n",
       "        ...,\n",
       "        [-1.0087e-04, -1.0422e-01, -7.0711e-01,  ...,  8.9315e-02,\n",
       "         -4.0760e-01,  4.7180e-01],\n",
       "        [-2.8052e-01, -9.2481e-02, -7.7559e-01,  ..., -1.0110e-01,\n",
       "         -4.4745e-01,  4.2153e-01],\n",
       "        [-3.2615e-01, -2.9795e-02, -8.4383e-01,  ...,  8.4649e-02,\n",
       "         -4.9991e-01,  7.8660e-01]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just for testing\n",
    "model = FenModel()\n",
    "data = next(iter(train_dl))\n",
    "model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165cc35a-52ef-4176-87d0-b5a456329cf6",
   "metadata": {},
   "source": [
    "## training setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ecce9-e265-4f34-a5a0-cd6d2691d737",
   "metadata": {},
   "source": [
    "## training loop\n",
    "\n",
    "*(We don't need to do full epochs as they're too big and unnecessary!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "711886ac-44c1-4282-934b-e23411b13af4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, dataloader, iterations, device='cpu'):\n",
    "    train_losses = []\n",
    "    train_acc = []\n",
    "    \n",
    "    for i, data in tqdm(enumerate(dataloader), total=iterations):\n",
    "        model.train()\n",
    "        data = {k: v.to(device) for k, v in data.items()}\n",
    "        optimizer.zero_grad()\n",
    "        labels_pred = model(data)\n",
    "        batch_loss = F.binary_cross_entropy_with_logits(labels_pred, data['targets'])\n",
    "    \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        train_losses.append(batch_loss.item())\n",
    "    \n",
    "        labels_pred_binary = torch.zeros_like(data['targets'])\n",
    "        labels_pred_binary[labels_pred > 0] = 1.0\n",
    "        train_acc.append(torch.mean((labels_pred_binary == data['targets']).float()).item())\n",
    "    \n",
    "        if (i+1) % (iterations // 20) == 0:\n",
    "            print(f'Iteration: {i+1}, train_loss: {train_losses[-1]:.4f}, train_acc: {train_acc[-1]:.4f}')\n",
    "\n",
    "        if i == iterations:\n",
    "            return train_losses, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a515bfad-0e46-443e-8cb7-4cde49cae265",
   "metadata": {},
   "source": [
    "### validation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "451a0316-e5d9-4bf7-ae50-07e074480ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_val(model, dataloader, iterations, device='cpu'):\n",
    "    val_losses = []\n",
    "    val_acc = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=iterations):\n",
    "            data = {k: v.to(device) for k, v in data.items()}\n",
    "            optimizer.zero_grad()\n",
    "            labels_pred = model(data)\n",
    "            batch_loss = F.binary_cross_entropy_with_logits(labels_pred, data['targets'])\n",
    "            \n",
    "            val_losses.append(batch_loss.item())\n",
    "            labels_pred_binary = torch.zeros_like(data['targets'])\n",
    "            labels_pred_binary[labels_pred > 0] = 1.0\n",
    "            val_acc.append(torch.mean((labels_pred_binary == data['targets']).float()).item())\n",
    "            \n",
    "            if i == iterations:\n",
    "                return val_losses, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdce0c8-eacd-4417-bc48-c69f2eea4f11",
   "metadata": {},
   "source": [
    "## train and examine the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccd7a36-5459-499d-a2f7-2fcbc5ddfb54",
   "metadata": {},
   "source": [
    "### train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fec505a-7c75-4590-a902-d1b68c76d1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2008c20999024fe98c8077008ae1af3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5, train_loss: 0.4725, train_acc: 0.8892\n",
      "Iteration: 10, train_loss: 0.2792, train_acc: 0.9709\n",
      "Iteration: 15, train_loss: 0.1840, train_acc: 0.9716\n",
      "Iteration: 20, train_loss: 0.1320, train_acc: 0.9726\n",
      "Iteration: 25, train_loss: 0.1137, train_acc: 0.9708\n",
      "Iteration: 30, train_loss: 0.0958, train_acc: 0.9731\n",
      "Iteration: 35, train_loss: 0.0956, train_acc: 0.9707\n",
      "Iteration: 40, train_loss: 0.0955, train_acc: 0.9695\n",
      "Iteration: 45, train_loss: 0.0847, train_acc: 0.9741\n",
      "Iteration: 50, train_loss: 0.0852, train_acc: 0.9731\n",
      "Iteration: 55, train_loss: 0.0845, train_acc: 0.9715\n",
      "Iteration: 60, train_loss: 0.0839, train_acc: 0.9727\n",
      "Iteration: 65, train_loss: 0.0845, train_acc: 0.9711\n",
      "Iteration: 70, train_loss: 0.0784, train_acc: 0.9733\n",
      "Iteration: 75, train_loss: 0.0876, train_acc: 0.9702\n",
      "Iteration: 80, train_loss: 0.0857, train_acc: 0.9709\n",
      "Iteration: 85, train_loss: 0.0848, train_acc: 0.9713\n",
      "Iteration: 90, train_loss: 0.0828, train_acc: 0.9731\n",
      "Iteration: 95, train_loss: 0.0860, train_acc: 0.9722\n",
      "Iteration: 100, train_loss: 0.0840, train_acc: 0.9723\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1d7523ff3d43dfa4431221a1e13dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create model\n",
    "model = FenModel().to(DEVICE)\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# train model\n",
    "train_losses, train_acc = train_model(model, optimizer, train_dl, ITERATIONS, device=DEVICE)\n",
    "\n",
    "# get validation stats\n",
    "val_losses, val_acc = get_val(model, val_dl, ITERATIONS // 10, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13d72382-7210-4865-a703-8fb50ed409ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val loss: 0.0827, val acc: 0.9721\n"
     ]
    }
   ],
   "source": [
    "print(f'val loss: {torch.mean(torch.tensor(val_losses)).item():.4f}, val acc: {torch.mean(torch.tensor(val_acc)).item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2adbb5-7bac-4426-a394-a2b0f31324bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
